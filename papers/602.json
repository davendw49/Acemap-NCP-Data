{"date": "2020/03/20", "journal": "medrxiv", "authors": "Cheng Jin, Weixiang Chen, Yukun Cao, Zhanwei Xu, Xin Zhang, Lei Deng, Chuansheng Zheng, Jie Zhou, Heshui Shi, Jianjiang Feng", "title": "Development and Evaluation of an AI System for COVID-19", "type": "preprint article", "abstract": "Early detection of COVID-19 based on chest CT will enable timely treatment of patients and help control the spread of the disease. With rapid spreading of COVID-19 in many countries, however, CT volumes of suspicious patients are increasing at a speed much faster than the availability of human experts. Here, we propose an artificial intelligence (AI) system for fast COVID-19 diagnosis with an accuracy comparable to experienced radiologists. A large dataset was constructed by collecting 970 CT volumes of 496 patients with confirmed COVID-19 and 260 negative cases from three hospitals in Wuhan, China, and 1,125 negative cases from two publicly available chest CT datasets. Trained using only 312 cases, our diagnosis system, which is based on deep convolutional neural network, is able to achieve an accuracy of 94.98%, an area under the receiver operating characteristic curve (AUC) of 97.91%, a sensitivity of 94.06%, and a specificity of 95.47% on an independent external verification dataset of 1,255 cases. In a reader study involving five radiologists, only one radiologist is slightly more accurate than the AI system. The AI system is two orders of magnitude faster than radiologists and the code is available at https : ==github:com=ChenW W W eixiang=diagnosis_covid19.", "text": "1        The new coronavirus disease, now known as COVID-19[                It is important to diagnosis COVID-19 as quick and accurate as possiblefor controlling the spread of the disease and treating patients. Even thoughreverse transcription-polymerase chain reaction (RT-PCR) is still ground truthof COVID-19 diagnosis, the sensitivity of RT-PCR is not high enough for lowviral load present in test specimens or laboratory error[                Artificial intelligence (AI) may be the unique preparation to take up thischallenge. Powered by large labeled datasets[                As a very recent disease, we have not yet found AI studies forCOVID19 diagnosis in peer-reviewed publications, but a few reports aboutCOVID19 diagnosis algorithms based on chest CT in preprint form[        In this study, we used clinically representative large-scale datasets from threecenters in Wuhan and two publicly available chest CT datasets to develop andevaluate an AI system for the diagnosis of COVID-19. We compared thediagnostic performance of this system with that of five radiologists in a reader studyof 200 cases, and the results showed the performance of this system iscomparable to that of experienced radiologists in designated hospitals for COVID-19in major epidemic areas. In addition, based on prediction score on every slice,we located the lesion areas in COVID-19 patients and statically analyzedlesion position in different subsets. We traced the specific phenotypic basis ofthe diagnosis output from the system on the original CT images through aninterpretation network and apply radiomics analysis of the attentional regionto understand the imaging characteristics of COVID-19. Figure 1 a shows theoverall flow of the study.22.1          We developed and evaluated a deep learning based COVID-19 diagnosis system,using multi-center data, which includes 756 subjects (496 COVID-19 positivesand 260 negatives) collected in Wuhan Union Hospital, Western Campus ofWuhan Union Hospital, and Jianghan Mobile Cabin Hospital in Wuhan, themajor epidemic area in China (detailed information is in Table 1). Twointernational public databases, LIDC-IDRI (1,012 subjects available)[          All CT data was divided into four independent parts with no overlappingsubjects, a 312 subjects training cohort, a 104 subjects internal validationcohort, an external test cohort consist of 1,255 subjects, and a reader study cohortwith 200 subjects (detailly described in Methods). Due to some positive subjectshave multi-stage CTs and different stage CTs of one subject might be similar,the subset division was performed on subjects to make sure all multi-stage CTsof the same subject are in a same subset.2.2We propose a deep-learning based AI system for COVID-19 diagnosis, whichdirectly takes CT data as input, performs lung segmentation, COVID-19diagnosis and abnormal slices locating. In addition, we hope that the diagnosisresults of AI system can be quantitatively explained in the original image toalleviate the drawback of deep neural networks as a black box. The systemconsists of five key components (Figure 1 a), (1) a 2D convolutional neural network(CNN) model for segmenting the lung, (2) a COVID-19 diagnosis model, (3) anabnormal slices locating block to locate abnormal slices in positive cases, (4) anetwork visualization module for interpreting the attentional region of deepnetworks, and (5) an image phenotype analysis module for explaining the featuresof the attentional region.The workflow of deep-learning based diagnosis model is shown in Figure 1 b.CT cases were firstly divided to different cohorts and extracted to slices sinceour model takes 2D slices as input. Then after slice level training, our modelcan accurately predict whether the input slices come from COVID-19 subjects.With a top-k average block, our model finally fused slice results into case leveldiagnosis. The model was implemented in 2D not only because 2D network waseasily to train with more training samples, but also because slice-level scorescan be used for abnormal slice locating. We fine-tuned our diagnosis modelon a training dataset consisting of normal and abnormal slices from COVID-19positive cases and obtained the abnormal slice locating model. Other parts ofour system are described in Methods.2.3The trained AI system was evaluated on the external test cohort. We used thereceiver operating characteristic (ROC) curves (in Figure 2 a) to evaluate thediagnostic accuracy of the proposed AI system. The PR curves of evaluatingthe prediction accuracy were also illustrated in Figure 7 a.On the external test cohort, the ROC curve showed AUC of 0.9791,sensitivity of 0.9406, and specificity of 0.9547. In the open data set LIDC-IDRI andILD-HUG, the false positive rates of AI system were 3.12% and 11.85%, andthe system showed good generalization ability (Figure 7 c).The AI system shows good performances and it can be used with differentdiagnosis thresholds according to different policies or prior probabilities. Thesensitivity of our system is about 84.76% when specificity is 99.5%, andspecificity is 80.02% when sensitivity is 97%. Besides, because patients in our externaltest cohort have multi-stage CT volumes, some of the stages of positivesubjects might be in the recovery state whose CT may have no abnormalities butare still regarded as positive in experiments. Figure 2 a shows the results afterroughly filtering out these cases by only keeping the maximum predicted valueof multi-stage CTs in the same patient, in which the specificity is about 96.74%at sensitivity of 97%. The decision curve analysis (DCA) for the AI system arepresented in Figure 7 b, which indicated that the AI system adds benefit thanthe \"diagnose all\" or \"diagnose none\" strategies when the threshold is within awide range 1.82-97.6% in COVID-19.Abnormal slice locating results are showed in Figure 2 c, d. The slice locatingblock took in COVID-19 positive cases and predicted where abnormal slices arewith AUC of 96.35%, specificity of 90.08% and sensitivity of 82.19%.2.4We conducted a reader study with five board-certified radiologists (Average of8 years clinical experience, range 5-11 years, Table 2 a). These radiologistsinterpreted 200 CT volumes in reader study cohort. All readers were asked toread independently without other information about patients.The AI system performs slightly better than the average of five radiologists.The ROC curve had AUC of 0. 9805, sensitivity of 0. 9470, and specificity of 0.9139 on the cohort of reader study (Figure 2 b, d). In 46% (6/13) of cases, whenthe AI system missed, the radiologist also missed (Table 2 b), indicating thatthe diagnosis of these missed cases is challenging. Among the five readers, onereader performed better than the AI system, one reader performed worse, andthe rest three have similar performance as the AI system at different operatingpoints. Performance of the AI system in COVID-19 diagnosis compared to fivereaders is shown in Figure 2 b and Table 2 c.The left, middle of Figure 2 e shows two COVID-19 cases that all fiveradiologists missed but were correctly identified by the AI system. It is hard to noticethe evidence of COVID-19 in these two cases, but AI system can still identifythem. It shows that the AI system may capture the information of subtle andcontinuous changes in slices and make a comprehensive judgment. The rightof Figure 2 e shows an example that was detected by all five radiologists butmissed by the AI system. These cases show that the AI system and humanreaders are potentially complementary.2.5For an in-depth understanding of the AI system and characteristics of differentpopulations with COVID-19, we evaluated the AI system on subsets of theexternal test cohort divided by gender, age and number of CT scans. Figure 3a shows the ROC curves of these three subsets. To understand the cause fordifferent diagnosis performances, we analyzed the abnormal slice locating resultsin different subsets (Figure 3 b, c). We found that the different performances ofdifferent subsets were highly correlated to the number of abnormal slices, whichmeans smaller lesion with fewer abnormal slices are more difficult to diagnose(Figure 3 c,Figure 4 b). Together with the position of abnormal slices andthe voxel numbers of lungs (Figure 4 a), we concluded that reason for worseperformance between 20 and 40 years old might be that younger people mayhave smaller lesions and less abnormal slices, while the worse performance onwomen might come from the smaller lungs and lesions.          Part of the patients in the database have multi-stage CTs. We comparedthe diagnostic performance of stage I and stage II and fusion of stage I, II inthe external test cohort (Figure 3 a, b, d). The experiment suggested thatthe performance of the AI system is independent of the progress of the diseasebecause of no significant differences between performances of different stages.The statistical results also showed that fusion of stage I and II could slightlyimprove the performance of diagnosis, in which the fusion method we adopted isto simply average the scores of two stages. We did not test more complex fusionmethods which may overestimate the performance since each negative case hasonly one CT.After proper training of the deep network, Guided gradient-weighted ClassActivation Mapping (Guided Grad-CAM)[                    In order to further verify our conjecture, we performed radiomics[          In this study, we developed an AI system for diagnosis of COVID-19. Thesystem showed good sensitivity (94.06%), specificity (95.47%) and AUC (97.91%)in external test cohort. Furthermore, in the reader study, the diagnosticaccuracy of the AI system was comparable to that of experienced radiologists from theoutbreak center, who achieved higher sensitivity (94.70%), specificity (91.39%)and AUC (98.05%). Among the five professional readers in the radiologydepartment, only one was able to produce a higher diagnostic accuracy than theAI system.This automatic, high-precision, non-invasive diagnostic system wasdeveloped to provide clinicians with easy-to-use tools. Given the chest CT of a suspectedpatient as input, the AI system can automatically output the diagnosis result.In the reader study, the average reading time of radiologists was 6.5 min, whilethat of AI system was 2.73 s, which can significantly improve the productivityof radiologists. Meanwhile, we found that 71% (15/21) of errors made byradiologists could be corrected by AI system. It means that AI system can beused as an effective secondary reader to provide reference suggestions when theradiologist is not sure about the case or when multiple radiologists areinconsistent. In general, AI can be adapted to different requirements. According to thehighly sensitive settings, it can screen out suspicious patients for confirmationby doctors; In accordance with the highly specific settings, it can warn possiblediagnosis errors made by the doctor; or an optimal threshold value is chosenaccording to the prior probability of infectious diseases and the local preventionand control strategy.        To further understand the performance of the AI system, we evaluated it onsubsets divided by gender, age and number of CT scans. In the subsets dividedby gender, the diagnostic performance of men was higher than that of women.We noticed an obvious difference in the size of lungs and lesions between men andwomen. This is consistent with the conclusion of Xiong et al.[        Further, we provided a visual explanation of the system\u2019s decision byperforming a radiomics analysis to obtain diagnostically relevant phenotypiccharacteristics of the attentional regions that are fully traceable on the original CTimage. This is important for an in-depth study of pulmonary imaging findingsin patients with COVID-19. For the AI system, by visualizing the diagnosticresults of 200 subjects from the reader study cohort and comparing them withhuman reader in the reader study, together with the subsequent radiomicsanalysis, we were able to perform detailed imaging phenotype analysis on thediagnosis of COVID-19, and subsequently make pathophysiological and anatomicalspeculations on the viral infection process (see Feature Analysis in Methods).There are still some drawbacks and future works of this research. First,collecting more data on other types of viral pneumonias or lung lesions canhelp improve its specificity further. Second, based on many chest CTs withdetailed labelled lesions, a semantic segmentation algorithm can be trained tolocate the outline of the lesion more accurately than Guided Grad-GAM, anddistinguish the detailed category of the lesion. Overall, the proposed AI systemhas been comprehensively validated on large dataset with diagnosis performancecomparable to human experts in diagnosing COVID-19. Unlike classicalblackbox deep learning approaches, by visualizing AI system and applying radiomicsanalysis, it can decode effective representation of COVID-19 on CT imaging,and potentially lead to the discovery of new biomarkers. Radiologists couldperform an individualized diagnosis of COVID-19 with the AI system, addingnew driving force for fighting the global spread of outbreak.44.1Under insitutional review board (IRB) approval , data used in our experimentscome from three centers in Wuhan, which are Wuhan Union hospital, WesternCampus of Wuhan Union Hospital, and Jianghan Mobile Cabin Hospital, andtwo public databases, LIDC-IDRI of the American National Cancer Institute(NCI), and ILD-HUG data of University Hospitals of Geneva.The datasets from the three centers in Wuhan contain both positive(confirmed COVID-19) and negative cases. Database collected from Jianghan MobileCabin Hospital in Wuhan includes chest CT volumes of patients with confirmedCOVID-19 from February 5th, 2020 to February 29th, 2020. Chest CT volumesfrom Wuhan Union hospital, Western Campus of Wuhan Union Hospital arecollected from January 11th, 2020 to February 29th, 2020. Data from threecenters contain 756 subjects, of which 496 were positives and 260 negatives.Some positive subjects have multi-stage CTs, so that datasets from threecenters consist of 710 CT volumes of positive cases and 260 CT volumes of negativecases. Stage I means the first chest CT of a patient, and the intervals to stagesII are 4-10 days.          Two public databases contain only negative cases since these data werecollected before COVID-19 outbreak. The Lung Image Database ConsortiumImage collection (LIDC-IDRI) is a collaboration between seven academic centersand eight medical imaging companies initiated by the national cancer institute(NCI) in the United States. This database contains 1,012 subjects[          All CT data was divided into four independent parts with no overlappingsubjects:Training cohort: 312 subjects were assigned to the training cohort,including 147 positive and 75 negative cases from three centers in Wuhan, and75 cases from LIDC-IDRI and 15 cases from ILD-HUG). This cohort wasused to train parameters of model.Internal validation cohort: 104 subjects were assigned to the internalvalidation cohort, including 49 positive and 25 negative cases from threecenters in Wuhan, and 25 cases from LIDC-IDRI and 5 cases fromILDHUG. This cohort was used to validate the performances and turnsuperparameters of model.External test cohort: There were 183 positive and 116 negative cases fromthree centers in Wuhan, 873 cases from LIDC-IDRI and 83 cases fromILD-HUG. To test performances for different genders and ages, we usedthis cohort excluded data of public databases. To test the performancesof different stage, we used 123 patients with 2 stages and all the negativecases collected in Wuhan in this cohort. This cohort was used to evaluateand analyze performances of AI system.Reader study cohort: 200 subjects (117 positive and 44 negative casesfrom three centers in Wuhan, and 39 cases from LIDC-IDRI database)were assigned to the reader study cohort. This cohort was used to comparediagnosis results with human radiologists.4.2The proposed AI system takes as input a whole CT volume and outputsCOVID19 diagnosis along with abnormal slices (if diagnosed as positive). The wholesystem consists of five parts: lung segmentation block, COVID-19 classificationnetwork, abnormal slice locating block for COVID-19 positives, AI systeminterpreting block using Guided Grad-CAM, and feature analysis block. The firstfour blocks are deep-learning based blocks and the last one is traditional featureextraction block.4.3          The lung segmentation block is implemented based on Deeplab v1[                    Our COVID-19 diagnosis block is a 2D classification deep network whosebackbone is ResNet152[          To measure the performance of classification model, AUC and some othermetrics are computed on both internal validation and external test cohort. Themetrics of internal validation cohort is computed on slice-level because thetraining is on slice-level and turning of super-parameters will be easier if validation isalso done on slice-level. While metrics for external test cohort and reader studycohort are on case level, consistent with clinical application.Block to locate abnormal slices is in the same structure of diagnosis blockbut trained especially on COVID-19 positive cases whose lesions have beenmarked manually. We used 28 cases with slice-level annotations in trainingcohort to train the block and the rest 12 cases in internal validation cohort withannotations to test performances of locating.We used Guided Grad-CAM to obtain attentional regions as our systeminterpreting block. Guided Grad-CAM has the advantage that it not onlygenerates a heat map to locate the relevant area, but also produces a coarselocalization map highlighting the important regions in the image for predicting theresult. Generally, the features used for classification judgment can be found,such as edges and spots in specific areas. Guided Grad-CAM is importantbecause the areas it focused on are a secondary output of our system togetherwith diagnosis result, giving more detailed diagnosis suggestions. Also, theattentional regions were used in latter feature extraction and analysis to get moredetailed information about lesion areas. We extracted region of attention bybinarizing output of Grad-CAM and then some morphological operations weredone on binarization map.          All the deep learning blocks were implemented using PyTorch[          Features were extracted in the attentional region determined by GuidedGradCAM. We also extracted the same feature in normal lung in controlled casesfor comparison. Due to no valid lesions attentional region for controlled casesis computed by Guided Grad-CAM, we used the shape of attentional region ofCOVID-19 cases and randomly choose positions within lung area as theattentional regions of controlled cases. We did not use shape features because theshape of attentional regions between COVID-19 and controlled cases are thesame.          We extracted radiomics features which are widely used in lesion diagnosisthese years. These features are composed of different image transforms andfeature matrix calculations. We adopted three image transforms: originalimage, transformed image by Laplacian of Gaussian (LoG) operator, andtransformed image by wavelet. For each image after the operation of a transform,six series of features are extracted, including first order features, Gray LevelCo-occurrence Matrix (GLCM), Gray Level Size Zone Matrix (GLSZM), GrayLevel Run Length Matrix (GLRLM), Neighboring Gray Tone Difference Matrix(NGTDM), Gray Level Dependence Matrix (GLDM). Radiomics analysis wasperformed using python version 3.6 and the \"pyradiomics\" package[                    First, we located the distribution of the attentional region traced by the AIsystem, which mainly consisted of the subpleural distribution, the fragmentarydistribution of patchy based on the secondary lobules, and the diffusedistribution of the fusing above two. The distances feature shows that the centersCluster heatmap of 15 radiomics features after LASSO. Note: In label column, redmeans COVID-19 positives while blue means negatives.of attentional region are generally 0-20 pixels (2.5 mm/pixel) from thepleura (Figure 8 a), which is consistent with anatomical findings on COVID-19.The SARS-Cov2 is small (60-140 nm in diameter), and when inhaled throughthe airways, it mainly invades the deep bronchioles, causes inflammation of thebronchioles and their surroundings to damage alveolar[                    According to all the extracted features, we can describe in depth therelationship between the medical findings and typical patterns. I) Halo pattern andanti-halo pattern were easily formed in the attentional regions. The halo patternwas speculated to be that the lesions (mainly the central node of the lobular)infiltrated into the surrounding interstitium and developed the aggregation ofinflammatory cells in the interstitium. Anti-halo pattern is of the center of theground glass shadow, almost completely surrounding by the high-densityconsolidation. The appearance of this sign may be that the inflammatory repair isdominated by the edge, leading to the formation of a band shadow tending toconsolidation at the edge, while the central repair is relatively slow. II) Theattentional region presents pleural parallel signs. The formation mechanism wasspeculated as follows: when the SARS-Cov2 invaded the interstitium aroundthe alveoli, the lymphatic return direction was subpleural and interlobularsepta, and diffused into pleural side and bilateral interlobular septum[          Readers can choose any window of gray value and zoom in or out whenreading CT volumes using Slicer 4.10.2 software while our system used fixedsize recased images (224 x 224 x 35) with fixed gray value window (-1200, 700)for all volumes.We would like to acknowledge the radiologists participating the reader study.This study was supported by Zhejiang University special scientific research fundfor COVID-19 prevention and control.", "ref_list": [[], ["Laboratory testing for coronavirus disease 2019 (COVID-19) in suspected human cases: interim guidance, 2 March 2020"], ["Performance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT"], ["Correlation of Chest CT and RT-PCR Testing in Coronavirus Disease 2019 (COVID-19"], ["Radiological findings from 81 patients with COVID-19 pneumonia in Wuhan, China: a descriptive study"], ["Imagenet: A large-scale hierarchical image database"], ["Deep learning"], ["Imagenet classification with deep convolutional neural networks"], ["Towards realtime object detection with region proposal networks"], ["Corrigendum: Dermatologist-level classification of skin cancer with deep neural networks"], ["Deep Learning in Medical Image Analysis"], ["A guide to deep learning in healthcare"], ["High-performance medicine: the convergence of human and artificial intelligence"], ["End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography"], ["A deep learning algorithm using CT images to screen for Corona Virus Disease (COVID-19)"], ["Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography: a prospective study"], ["The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans"], ["Building a reference multimedia database for interstitial lung diseases"], ["Grad-cam: Visual explanations from deep networks via gradient-based localization"], ["Findings in 2019 Novel Coronavirus (2019-nCoV) Infections from Wuhan, China: Key Points for the Radiologist"], ["Computational Radiomics System to Decode the Radiographic Phenotype"], ["Relationship to Duration of Infection"], ["Women May Play a More Important Role in the Transmission of the Corona Virus Disease (COVID-19) than Men"], ["DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"], ["Deep residual learning for image recognition"], ["An imperative style, high-performance deep learning library"], ["Computational radiomics system to decode the radiographic phenotype"], [""], ["and"], ["Lung pathology of fatal severe acute respiratory syndrome"], ["Origin and evolution of pathogenic coronaviruses"], ["Epidemiology, genetic recombination, and pathogenesis of coronaviruses"], ["Lung function: physiology, measurement and application in medicine"], ["Pulmonary vascular and cardiac impairment in interstitial lung disease"], ["Lymphatic Vessel Network Structure and Physiology"], ["Inflammatory mechanisms in the lung"], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], "ref_authors": [[], [], ["H. X."], ["T."], [], [], ["Y. Bengio", "G. J. n. Hinton"], ["A. Sutskever", "I. Hinton", "G. E."], ["S. He", "K. Girshick", "R. Sun", "J. Faster"], ["A."], ["D. Wu", "G. Suk", "H. I."], ["A."], ["E. J."], ["D."], ["S."], [], ["S. G."], ["A."], ["R. R."], ["J. P. Chest", "CT"], ["J. J. M. Griethuysen"], ["A."], ["Q."], ["L. C. Papandreou", "G. Kokkinos", "I. Murphy", "K. Yuille", "A. L."], ["K. Zhang", "X. Ren", "S. Sun", "J."], ["P."], ["J. J. Griethuysen"], ["M. Rajnik", "M. Cuomo", "A. Features"], ["D. H.", "S. P. Hammar", "Dail"], ["J. M."], ["J. Li", "F. Shi", "Z.-L. J. N. r. M."], ["S."], ["J. E. Chinn", "D. J. Miller", "M. R."], ["M. Church", "A. C. Johnson", "M. K. Peacock", "A. J."], ["J. W."], ["B."], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], "fir_para": "1", "one_words_summarize": "1        The new coronavirus disease, now known as COVID-19[                It is important to diagnosis COVID-19 as quick and accurate as possiblefor controlling the spread of the disease and treating patients. We traced the specific phenotypic basis ofthe diagnosis output from the system on the original CT images through aninterpretation network and apply radiomics analysis of the attentional regionto understand the imaging characteristics of COVID-19. CT cases were firstly divided to different cohorts and extracted to slices sinceour model takes 2D slices as input. With a top-k average block, our model finally fused slice results into case leveldiagnosis. Thesensitivity of our system is about 84.76% when specificity is 99.5%, andspecificity is 80.02% when sensitivity is 97%. The AI system performs slightly better than the average of five radiologists. Performance of the AI system in COVID-19 diagnosis compared to fivereaders is shown in Figure 2 b and Table 2 c. These cases show that the AI system and humanreaders are potentially complementary.2.5For an in-depth understanding of the AI system and characteristics of differentpopulations with COVID-19, we evaluated the AI system on subsets of theexternal test cohort divided by gender, age and number of CT scans. To understand the cause fordifferent diagnosis performances, we analyzed the abnormal slice locating resultsin different subsets (Figure 3 b, c). We did not test more complex fusionmethods which may overestimate the performance since each negative case hasonly one CT.After proper training of the deep network, Guided gradient-weighted ClassActivation Mapping (Guided Grad-CAM)[                    In order to further verify our conjecture, we performed radiomics[          In this study, we developed an AI system for diagnosis of COVID-19. It means that AI system can beused as an effective secondary reader to provide reference suggestions when theradiologist is not sure about the case or when multiple radiologists areinconsistent. First,collecting more data on other types of viral pneumonias or lung lesions canhelp improve its specificity further. Database collected from Jianghan MobileCabin Hospital in Wuhan includes chest CT volumes of patients with confirmedCOVID-19 from February 5th, 2020 to February 29th, 2020. Chest CT volumesfrom Wuhan Union hospital, Western Campus of Wuhan Union Hospital arecollected from January 11th, 2020 to February 29th, 2020. Data from threecenters contain 756 subjects, of which 496 were positives and 260 negatives. Two public databases contain only negative cases since these data werecollected before COVID-19 outbreak. Internal validation cohort: 104 subjects were assigned to the internalvalidation cohort, including 49 positive and 25 negative cases from threecenters in Wuhan, and 25 cases from LIDC-IDRI and 5 cases fromILDHUG. Also, theattentional regions were used in latter feature extraction and analysis to get moredetailed information about lesion areas. All the deep learning blocks were implemented using PyTorch[          Features were extracted in the attentional region determined by GuidedGradCAM. We also extracted the same feature in normal lung in controlled casesfor comparison. We extracted radiomics features which are widely used in lesion diagnosisthese years. These features are composed of different image transforms andfeature matrix calculations. Note: In label column, redmeans COVID-19 positives while blue means negatives.of attentional region are generally 0-20 pixels (2.5 mm/pixel) from thepleura (Figure 8 a), which is consistent with anatomical findings on COVID-19.The SARS-Cov2 is small (60-140 nm in diameter), and when inhaled throughthe airways, it mainly invades the deep bronchioles, causes inflammation of thebronchioles and their surroundings to damage alveolar[                    According to all the extracted features, we can describe in depth therelationship between the medical findings and typical patterns."}